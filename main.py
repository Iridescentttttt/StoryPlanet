from openai import OpenAI
from env import *  # ç¡®ä¿ api_key å­˜åœ¨
import gradio as gr
import json
from fpdf import FPDF
import os
import tempfile
import requests
from PIL import Image
from io import BytesIO
import base64
# from modelscope.pipelines import pipeline
# from modelscope.utils.constant import Tasks
# from modelscope.outputs import OutputKeys


# æ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–
client = OpenAI(
    base_url='https://api-inference.modelscope.cn/v1/',
    api_key=MODELSCOPE_TOKEN,
)

# ============== å·¥å…·å‡½æ•° =================
def image_to_base64(image_path):
    with open(image_path, "rb") as img_f:
        encoded = base64.b64encode(img_f.read()).decode("utf-8")
        ext = image_path.split('.')[-1]
        return f"data:image/{ext};base64,{encoded}"

def split_story_to_paragraphs(story_text):
    return [p.strip() for p in story_text.split('\n') if p.strip()]

# ============== å›¾åƒç”Ÿæˆ =================

IMAGE_GEN_URL = 'https://api-inference.modelscope.cn/v1/images/generations'
IMAGE_MODEL_ID = 'MusePublic/489_ckpt_FLUX_1'

def generate_image(prompt_text, save_path):
    headers = {
        'Authorization': f'Bearer {MODELSCOPE_TOKEN}',
        'Content-Type': 'application/json'
    }
    payload = {
        'model': IMAGE_MODEL_ID,
        'prompt': prompt_text
    }
    response = requests.post(IMAGE_GEN_URL, json=payload, headers=headers)
    if response.status_code != 200:
        raise Exception(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {response.text}")
    image_url = response.json()['images'][0]['url']
    image = Image.open(BytesIO(requests.get(image_url).content))
    image.save(save_path)
    return save_path

def summarize_and_translate(paragraphs):
    english_prompts = []
    for i, para in enumerate(paragraphs):
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¤ç‹¬ç—‡å„¿ç«¥ç»˜æœ¬å›¾åƒæç¤ºè¯­è®¾è®¡åŠ©æ‰‹ï¼Œè¯·å¯¹ä»¥ä¸‹ä¸­æ–‡æ•…äº‹æ®µè½è¿›è¡Œç®€è¦æ¦‚æ‹¬ï¼Œå¹¶ç¿»è¯‘æˆä¸€å¥è‹±æ–‡å›¾åƒæç¤ºã€‚è¦æ±‚ç®€ç»ƒæ¸…æ™°ï¼Œå†™æ˜å¡é€šé£æ ¼ï¼Œä¸è¦è§£é‡Šæˆ–åŠ æ ‡ç‚¹ï¼Œåªè¾“å‡ºè‹±æ–‡ã€‚

æ®µè½ï¼š
{para}

è¾“å‡ºè‹±æ–‡æè¿°ï¼š"""

        response = client.chat.completions.create(
            model='deepseek-ai/DeepSeek-V3',
            messages=[
                {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç»˜æœ¬å›¾åƒæç¤ºè¯ç”Ÿæˆå™¨ï¼Œåªè¾“å‡ºè‹±æ–‡å›¾åƒæè¿°ã€‚'},
                {'role': 'user', 'content': prompt}
            ]
        )
        english = response.choices[0].message.content.strip()
        english_prompts.append(english)
        print(f"ç¬¬{i+1}æ®µå†…å®¹ç¿»è¯‘å¹¶æ¦‚æ‹¬:{english}")
    return english_prompts

# ============== æ„å»ºç»˜æœ¬è¯·æ±‚ Prompt =================

def build_prompt(age, trait, interest, sensitivity, cognitive, edu_goal, words, max_characters,para_count):
    return f"""
ä½ æ˜¯å­¤ç‹¬ç—‡å„¿ç«¥ç»˜æœ¬åˆ›ä½œä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å„¿ç«¥ä¿¡æ¯å’Œæ•™è‚²ç›®æ ‡ï¼Œç”Ÿæˆä¸€ç¯‡é€‚åˆTAé˜…è¯»çš„æ•…äº‹ï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºã€‚

ã€å„¿ç«¥åŸºæœ¬ä¿¡æ¯ã€‘
- å¹´é¾„ï¼š{age}å²
- æ€§æ ¼ç‰¹ç‚¹ï¼š{trait}
- å…´è¶£ä¸»é¢˜ï¼š{interest}
- æ„Ÿå®˜æ•æ„Ÿç‚¹ï¼š{sensitivity}
- è®¤çŸ¥æ°´å¹³ï¼š{cognitive}

ã€æ•…äº‹ç”Ÿæˆè¦æ±‚
- æ•™è‚²ç›®æ ‡ï¼š{edu_goal}
- æœŸæœ›æ®µè½æ•°é‡ï¼š{para_count}æ®µ
- è§’è‰²æ•°é‡ä¸è¶…è¿‡{max_characters}ä¸ª
- å­—æ•°æ§åˆ¶åœ¨{words}å­—å·¦å³ï¼Œè¯­è¨€æ¸©å’Œï¼Œé¿å…æ„Ÿå®˜åˆºæ¿€
- æ¯æ®µä¿æŒé€»è¾‘æ¸…æ™°ï¼Œé€‚å½“ä½¿ç”¨é‡å¤å¥å¼
- è¾“å‡ºä¸ºä»¥ä¸‹ JSON æ ¼å¼ï¼ˆä¸åŠ  Markdownã€æ³¨é‡Šï¼‰ï¼š

è¯·ä¸¥æ ¼è¾“å‡ºå¦‚ä¸‹ JSON æ ¼å¼ï¼š

{{
  "title": "ç»˜æœ¬æ ‡é¢˜",
  "story": "ç»˜æœ¬æ­£æ–‡å†…å®¹ï¼Œçº¦{words}å­—ã€‚è¯·åˆç†åˆ†æ®µã€‚",
  "goal": "æ•™è‚²ç›®æ ‡ï¼ˆæ ‡ç­¾+è¯´æ˜ï¼‰",
  "parent_tip": "ç»™å®¶é•¿çš„å¼•å¯¼å»ºè®®",
  "qa": [
    {{"question": "é—®é¢˜1", "answer": "å›ç­”1"}},
    {{"question": "é—®é¢˜2", "answer": "å›ç­”2"}}
  ]
}}

è¯·ä¸¥æ ¼åªè¾“å‡º JSONï¼Œä¸åŠ ä»»ä½•æ³¨é‡Šã€æ ‡ç‚¹æˆ– Markdownã€‚
"""

def generate_story(age, trait, interest, sensitivity, cognitive, edu_goal, max_words, max_characters,para_count):
    prompt = build_prompt(age, trait, interest, sensitivity, cognitive, edu_goal, max_words, max_characters,para_count)
    response = client.chat.completions.create(
        model='deepseek-ai/DeepSeek-V3',
        messages=[
            {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªå­¤ç‹¬ç—‡å„¿ç«¥ç»˜æœ¬åˆ›ä½œä¸“å®¶ï¼Œè¾“å‡ºç»“æ„åŒ–JSONå†…å®¹ã€‚'},
            {'role': 'user', 'content': prompt}
        ],
    )
    print(f"Raw message:\n{response}\n")
    content = response.choices[0].message.content.strip()
    print(f"LLM Answer:\n {content}\n")
    try:
        result = json.loads(content)
        print(f"Json parser:\n {result}\n")
        return result
    except Exception as e:
        return {"error": f"è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ ¼å¼ã€‚é”™è¯¯ï¼š{e}", "raw_output": content}

# ============== PDF åˆ›å»ºå‡½æ•° =================

def create_pdf(data, mode="full", image_paths=None):
    pdf = FPDF()
    font_path = "SimHei.ttf"
    if not os.path.exists(font_path):
        raise FileNotFoundError("ç¼ºå°‘å­—ä½“ SimHei.ttf")

    pdf.add_page()
    pdf.add_font("SimHei", "", font_path)
    pdf.set_font("SimHei", size=14)
    pdf.multi_cell(0, 10, f"ç»˜æœ¬æ ‡é¢˜ï¼š{data['title']}\n", align="L")
    pdf.set_font("SimHei", size=12)

    paragraphs = split_story_to_paragraphs(data['story'])

    for idx, para in enumerate(paragraphs):
        pdf.multi_cell(0, 10, para)
        pdf.ln(2)

        # å›¾æ–‡ç‰ˆ æˆ– full å›¾æ–‡ç‰ˆæ—¶æ’å›¾
        if mode in ["illustrated", "full", "question"] and image_paths and idx < len(image_paths):
            img = Image.open(image_paths[idx])
            img.thumbnail((200, 200))
            thumb_path = f"thumb_{idx}.jpg"
            img.save(thumb_path)  # ğŸ‘ˆ ç¡®ä¿ä¿å­˜ç¼©ç•¥å›¾
            x_center = (210 - 100) / 2
            pdf.image(thumb_path, x=x_center, w=100)
            pdf.ln(5)

    if mode == "full":
        pdf.multi_cell(0, 10, f"\næ•™è‚²ç›®æ ‡ï¼š{data['goal']}\n")
        pdf.multi_cell(0, 10, f"å®¶é•¿å¼•å¯¼å»ºè®®ï¼š{data['parent_tip']}\n")
        for qa in data["qa"]:
            pdf.multi_cell(0, 10, f"é—®ï¼š{qa['question']}\nç­”ï¼š{qa['answer']}\n")

    elif mode == "question":
        for qa in data["qa"]:
            pdf.multi_cell(0, 10, f"é—®é¢˜ï¼š{qa['question']}\nç­”ï¼š__________________________\n\n")

    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
    pdf.output(tmp.name)
    return tmp.name

# ============== åˆ†æ­¥æ‰§è¡Œå‡½æ•° =================

def generate_story_only(age, trait, interest, sensitivity, cognitive, max_words, max_characters, para_count, edu_goal):
    data = generate_story(age, trait, interest, sensitivity, cognitive, edu_goal, max_words, max_characters, para_count)
    if "error" in data:
        return data["error"], "", "", "", "", data

    qa_str = "\n".join([f"Q: {q['question']}\nA: {q['answer']}" for q in data["qa"]])
    paragraphs = split_story_to_paragraphs(data["story"])
    story_html = ""
    for i, p in enumerate(paragraphs):
        story_html += f'<div id="para{i}"><p>{p}</p></div>'

    return data['title'], story_html, data['goal'], data['parent_tip'], qa_str, data

def async_generate_images_and_pdfs(data, progress=gr.Progress()):
    paragraphs = split_story_to_paragraphs(data['story'])
    english_prompts = summarize_and_translate(paragraphs)

    story_html_parts = []
    image_paths = []

    # åˆå§‹çº¯æ–‡å­—
    for i, para in enumerate(paragraphs):
        story_html_parts.append(f'<div id="para{i}"><p>{para}</p></div>')

    yield "\n".join(story_html_parts), "å¼€å§‹ç”Ÿæˆæ’å›¾...", None, None, None, None

    for idx, prompt in enumerate(progress.tqdm(english_prompts, desc="ç”Ÿæˆæ’å›¾ä¸­...")):
        img_path = f"illustration_{idx + 1}.jpg"
        try:
            generate_image(prompt, img_path)
            img = Image.open(img_path)
            img.thumbnail((500, 500))
            img.save(img_path)
            image_paths.append(img_path)

            # æ’å…¥å›¾åƒ HTML åˆ°å¯¹åº”æ®µè½
            # story_html_parts[idx] = story_html_parts[idx].replace(
            #     "</div>", f'<img src="file/{img_path}" style="max-width:100%; margin-top:5px;"></div>'
            # )
            base64_src = image_to_base64(img_path)
            story_html_parts[idx] = story_html_parts[idx].replace(
                "</div>",
                f'<img src="{base64_src}" style="max-width:100%; margin-top:5px;"></div>'
            )

        except Exception as e:
            story_html_parts[idx] += f"<p style='color:red;'>æ’å›¾å¤±è´¥ï¼š{e}</p>"

        yield "\n".join(story_html_parts), f"âœ… å·²å®Œæˆ {idx+1}/{len(english_prompts)} å¼ æ’å›¾", None, None, None, None

    # å››ç§ PDF åˆ†åˆ«ç”Ÿæˆ
    pdf_story = create_pdf(data, mode="story")  # ä»…æ­£æ–‡ï¼Œæ— å›¾
    pdf_full = create_pdf(data, mode="full", image_paths=image_paths)  # å…¨éƒ¨æ–‡å­—ä¿¡æ¯ï¼Œæ— å›¾
    pdf_qa = create_pdf(data, mode="question", image_paths=image_paths)  # é—®ç­”ç»ƒä¹ ï¼Œæ— å›¾
    pdf_illustrated = create_pdf(data, mode="illustrated", image_paths=image_paths)  # å›¾æ–‡ç‰ˆï¼Œåªæ­£æ–‡+å›¾

    audio_path = synthesize_audio_from_story(data['story'])
    yield "\n".join(story_html_parts), "âœ… æ‰€æœ‰æ’å›¾å·²å®Œæˆ", pdf_story, pdf_full, pdf_qa, pdf_illustrated, audio_path


# ============== è¯­éŸ³åˆæˆ =================
def synthesize_audio_from_story(story_text):
    from datetime import datetime

    # åˆå§‹åŒ–è¯­éŸ³æ¨¡å‹
    tts_pipeline = pipeline(
        task=Tasks.text_to_speech,
        model='damo/speech_sambert-hifigan_tts_zh-cn_16k'
    )

    # æ‰§è¡Œè¯­éŸ³åˆæˆ
    output = tts_pipeline(input=story_text, voice='zhitian_emo')
    wav_data = output[OutputKeys.OUTPUT_WAV]

    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    tmp_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
    with open(tmp_path, 'wb') as f:
        f.write(wav_data)

    return tmp_path

# ============== Gradio UI =================

with gr.Blocks(title="å›¾æ–‡ç»˜æœ¬ç”Ÿæˆç³»ç»Ÿ") as demo:
    gr.Markdown("## ğŸ¨ å­¤ç‹¬ç—‡å„¿ç«¥å›¾æ–‡ç»˜æœ¬ç”Ÿæˆå™¨")

    with gr.Row():
        with gr.Column():
            age = gr.Slider(2, 18, label="å„¿ç«¥å¹´é¾„", value=2)
            max_words = gr.Slider(50, 1500, label="æœ€å¤§å­—æ•°", value=50)
            para_count = gr.Slider(1, 10, step=1, label="æ®µè½æ•°é‡", value=1)
            max_characters = gr.Slider(1, 5, label="æœ€å¤šè§’è‰²", value=1)
            trait = gr.Textbox(label="æ€§æ ¼ç‰¹ç‚¹", value="æ— ")
            interest = gr.Textbox(label="å…´è¶£ä¸»é¢˜", value="æ— ")
            sensitivity = gr.Textbox(label="æ„Ÿå®˜æ•æ„Ÿç‚¹", value="æ— ")
            cognitive = gr.Textbox(label="è®¤çŸ¥æ°´å¹³", value="æ— ")
            edu_goal = gr.Textbox(label="æ•™è‚²ç›®æ ‡", value="æ— ")
            generate_btn = gr.Button("ç”Ÿæˆå›¾æ–‡ç»˜æœ¬")

        with gr.Column():
            title_box = gr.Textbox(label="ç»˜æœ¬æ ‡é¢˜")
            story_display = gr.HTML(label="ç»˜æœ¬å›¾æ–‡å†…å®¹")
            goal_box = gr.Textbox(label="æ•™è‚²ç›®æ ‡")
            tip_box = gr.Textbox(label="å®¶é•¿å¼•å¯¼å»ºè®®")
            qa_box = gr.Textbox(label="ç†è§£é—®ç­”", lines=4)
            image_progress = gr.Textbox(label="ç”Ÿæˆè¿›åº¦", interactive=False)
            # audio_output = gr.Audio(label="ğŸ§ æ•…äº‹æœ—è¯»éŸ³é¢‘", type='filepath')

    with gr.Row():
        pdf_story = gr.File(label="ğŸ“˜ æ­£æ–‡PDF")
        pdf_full = gr.File(label="ğŸ“• å®Œæ•´PDF")
        pdf_qa = gr.File(label="ğŸ§  ç»ƒä¹ PDF")
        pdf_illustrated = gr.File(label="ğŸ¨ å›¾æ–‡PDF")

    intermediate_state = gr.State()

    generate_btn.click(
        fn=generate_story_only,
        inputs=[age, trait, interest, sensitivity, cognitive, max_words, max_characters, para_count, edu_goal],
        outputs=[title_box, story_display, goal_box, tip_box, qa_box, intermediate_state]
    )

    intermediate_state.change(
        fn=async_generate_images_and_pdfs,
        inputs=[intermediate_state],
        outputs=[story_display, image_progress, pdf_story, pdf_full, pdf_qa, pdf_illustrated],
        show_progress=True
    )

    # intermediate_state.change(
    #     fn=async_generate_images_and_pdfs,
    #     inputs=[intermediate_state],
    #     outputs=[
    #         story_display, image_progress,
    #         pdf_story, pdf_full, pdf_qa, pdf_illustrated,
    #         audio_output
    #     ],
    #     show_progress=True
    # )

    demo.launch()