from openai import OpenAI
from env import *  # ç¡®ä¿ api_key å­˜åœ¨
import gradio as gr
import json
from fpdf import FPDF
import os
import tempfile
import requests
from PIL import Image
from io import BytesIO
import time
import base64

def image_to_base64(image_path):
    with open(image_path, "rb") as img_f:
        encoded = base64.b64encode(img_f.read()).decode("utf-8")
        ext = image_path.split('.')[-1]
        return f"data:image/{ext};base64,{encoded}"


# æ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–
client = OpenAI(
    base_url='https://api-inference.modelscope.cn/v1/',
    api_key=MODELSCOPE_TOKEN,
)

# ============== å·¥å…·å‡½æ•° =================

def split_story_to_paragraphs(story_text):
    return [p.strip() for p in story_text.split('\n') if p.strip()]

# ============== å›¾åƒç”Ÿæˆ =================

IMAGE_GEN_URL = 'https://api-inference.modelscope.cn/v1/images/generations'
IMAGE_MODEL_ID = 'MusePublic/489_ckpt_FLUX_1'

def generate_image(prompt_text, save_path):
    headers = {
        'Authorization': f'Bearer {MODELSCOPE_TOKEN}',
        'Content-Type': 'application/json'
    }
    payload = {
        'model': IMAGE_MODEL_ID,
        'prompt': prompt_text
    }
    response = requests.post(IMAGE_GEN_URL, json=payload, headers=headers)
    if response.status_code != 200:
        raise Exception(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {response.text}")
    image_url = response.json()['images'][0]['url']
    image = Image.open(BytesIO(requests.get(image_url).content))
    image.save(save_path)
    return save_path

def summarize_and_translate(paragraphs):
    english_prompts = []
    for i, para in enumerate(paragraphs):
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¤ç‹¬ç—‡å„¿ç«¥ç»˜æœ¬å›¾åƒæç¤ºè¯­è®¾è®¡åŠ©æ‰‹ï¼Œè¯·å¯¹ä»¥ä¸‹ä¸­æ–‡æ•…äº‹æ®µè½è¿›è¡Œç®€è¦æ¦‚æ‹¬ï¼Œå¹¶ç¿»è¯‘æˆä¸€å¥è‹±æ–‡å›¾åƒæç¤ºã€‚è¦æ±‚ç®€ç»ƒæ¸…æ™°ï¼Œå†™æ˜å¡é€šé£æ ¼ï¼Œä¸è¦è§£é‡Šæˆ–åŠ æ ‡ç‚¹ï¼Œåªè¾“å‡ºè‹±æ–‡ã€‚

æ®µè½ï¼š
{para}

è¾“å‡ºè‹±æ–‡æè¿°ï¼š"""

        response = client.chat.completions.create(
            model='deepseek-ai/DeepSeek-V3',
            messages=[
                {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç»˜æœ¬å›¾åƒæç¤ºè¯ç”Ÿæˆå™¨ï¼Œåªè¾“å‡ºè‹±æ–‡å›¾åƒæè¿°ã€‚'},
                {'role': 'user', 'content': prompt}
            ]
        )
        english = response.choices[0].message.content.strip()
        english_prompts.append(english)
        print(f"ç¬¬{i+1}æ®µå†…å®¹ç¿»è¯‘å¹¶æ¦‚æ‹¬:{english}")
    return english_prompts

# ============== æ„å»ºç»˜æœ¬è¯·æ±‚ Prompt =================

def build_prompt(age, trait, interest, sensitivity, cognitive, edu_goal, words, max_characters):
    return f"""
ä½ æ˜¯å­¤ç‹¬ç—‡å„¿ç«¥ç»˜æœ¬åˆ›ä½œä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å„¿ç«¥ä¿¡æ¯å’Œæ•™è‚²ç›®æ ‡ï¼Œç”Ÿæˆä¸€ç¯‡é€‚åˆTAé˜…è¯»çš„æ•…äº‹ï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºã€‚

ã€å„¿ç«¥åŸºæœ¬ä¿¡æ¯ã€‘
- å¹´é¾„ï¼š{age}å²
- æ€§æ ¼ç‰¹ç‚¹ï¼š{trait}
- å…´è¶£ä¸»é¢˜ï¼š{interest}
- æ„Ÿå®˜æ•æ„Ÿç‚¹ï¼š{sensitivity}
- è®¤çŸ¥æ°´å¹³ï¼š{cognitive}

ã€æ•…äº‹ç”Ÿæˆè¦æ±‚ã€‘
- å­—æ•°æ§åˆ¶åœ¨{words}å­—å·¦å³
- è§’è‰²æ•°é‡ä¸è¶…è¿‡{max_characters}ä¸ª
- åˆé€‚ä½ç½®è¯·æ¢æ®µè½ï¼Œå¸®åŠ©ç†è§£
- ä½¿ç”¨é‡å¤å¥å¼
- é¿å…åˆºæ¿€æ€§è¯æ±‡
- åŒ…å«æ•™è‚²ç›®æ ‡è¯´æ˜

ã€æ•™è‚²ç›®æ ‡è¯´æ˜ã€‘
{edu_goal}

è¯·ä¸¥æ ¼è¾“å‡ºå¦‚ä¸‹ JSON æ ¼å¼ï¼š

{{
  "title": "ç»˜æœ¬æ ‡é¢˜",
  "story": "ç»˜æœ¬æ­£æ–‡å†…å®¹ï¼Œçº¦{words}å­—ã€‚è¯·åˆç†åˆ†æ®µã€‚",
  "goal": "æ•™è‚²ç›®æ ‡ï¼ˆæ ‡ç­¾+è¯´æ˜ï¼‰",
  "parent_tip": "ç»™å®¶é•¿çš„å¼•å¯¼å»ºè®®",
  "qa": [
    {{"question": "é—®é¢˜1", "answer": "å›ç­”1"}},
    {{"question": "é—®é¢˜2", "answer": "å›ç­”2"}}
  ]
}}

è¯·ä¸¥æ ¼åªè¾“å‡º JSONï¼Œä¸åŠ ä»»ä½•æ³¨é‡Šã€æ ‡ç‚¹æˆ– Markdownã€‚
"""

def generate_story(age, trait, interest, sensitivity, cognitive, edu_goal, max_words, max_characters):
    prompt = build_prompt(age, trait, interest, sensitivity, cognitive, edu_goal, max_words, max_characters)
    response = client.chat.completions.create(
        model='deepseek-ai/DeepSeek-V3',
        messages=[
            {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªå­¤ç‹¬ç—‡å„¿ç«¥ç»˜æœ¬åˆ›ä½œä¸“å®¶ï¼Œè¾“å‡ºç»“æ„åŒ–JSONå†…å®¹ã€‚'},
            {'role': 'user', 'content': prompt}
        ],
    )
    print(f"Raw message:\n{response}\n")
    content = response.choices[0].message.content.strip()
    print(f"LLM Answer:\n {content}\n")
    try:
        result = json.loads(content)
        print(f"Json parser:\n {result}\n")
        return result
    except Exception as e:
        return {"error": f"è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ ¼å¼ã€‚é”™è¯¯ï¼š{e}", "raw_output": content}

# ============== PDF åˆ›å»ºå‡½æ•° =================

def create_pdf(data, mode="full", image_paths=None):
    pdf = FPDF()
    font_path = "SimHei.ttf"
    if not os.path.exists(font_path):
        raise FileNotFoundError("ç¼ºå°‘å­—ä½“ SimHei.ttf")

    pdf.add_page()
    pdf.add_font("SimHei", "", font_path)
    pdf.set_font("SimHei", size=14)
    pdf.multi_cell(0, 10, f"ç»˜æœ¬æ ‡é¢˜ï¼š{data['title']}\n", align="L")
    pdf.set_font("SimHei", size=12)

    paragraphs = split_story_to_paragraphs(data['story'])

    for idx, para in enumerate(paragraphs):
        pdf.multi_cell(0, 10, para)
        pdf.ln(2)

        # å›¾æ–‡ç‰ˆ æˆ– full å›¾æ–‡ç‰ˆæ—¶æ’å›¾
        if mode in ["illustrated", "full", "question"] and image_paths and idx < len(image_paths):
            img = Image.open(image_paths[idx])
            img.thumbnail((200, 200))
            thumb_path = f"thumb_{idx}.jpg"
            # img.save(thumb_path)
            # pdf.image(thumb_path, x=pdf.w / 2 - 60, w=120)
            # pdf.ln(5)
            x_center = (210 - 100) / 2  # A4å®½åº¦210mmï¼Œå›¾å®½100ï¼Œå±…ä¸­
            pdf.image(thumb_path, x=x_center, w=100)
            pdf.ln(5)

    if mode == "full":
        pdf.multi_cell(0, 10, f"\næ•™è‚²ç›®æ ‡ï¼š{data['goal']}\n")
        pdf.multi_cell(0, 10, f"å®¶é•¿å¼•å¯¼å»ºè®®ï¼š{data['parent_tip']}\n")
        for qa in data["qa"]:
            pdf.multi_cell(0, 10, f"é—®ï¼š{qa['question']}\nç­”ï¼š{qa['answer']}\n")

    elif mode == "question":
        for qa in data["qa"]:
            pdf.multi_cell(0, 10, f"é—®é¢˜ï¼š{qa['question']}\nç­”ï¼š__________________________\n\n")

    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
    pdf.output(tmp.name)
    return tmp.name

# ============== åˆ†æ­¥æ‰§è¡Œå‡½æ•° =================

def generate_story_only(age, trait, interest, sensitivity, cognitive, max_words, max_characters, edu_goal_choice, edu_goal_custom):
    final_goal = edu_goal_custom if edu_goal_choice == "è‡ªå®šä¹‰ç›®æ ‡" else edu_goal_choice
    data = generate_story(age, trait, interest, sensitivity, cognitive, final_goal, max_words, max_characters)
    if "error" in data:
        return data["error"], "", "", "", "", data

    qa_str = "\n".join([f"Q: {q['question']}\nA: {q['answer']}" for q in data["qa"]])
    paragraphs = split_story_to_paragraphs(data["story"])

    # åˆå§‹åªæ˜¾ç¤ºæ–‡æœ¬ï¼Œæ¯æ®µä¸€ä¸ª <p> æ ‡ç­¾
    story_html = ""
    for i, p in enumerate(paragraphs):
        story_html += f'<div id="para{i}"><p>{p}</p></div>'

    return data['title'], story_html, data['goal'], data['parent_tip'], qa_str, data

def async_generate_images_and_pdfs(data, progress=gr.Progress()):
    paragraphs = split_story_to_paragraphs(data['story'])
    english_prompts = summarize_and_translate(paragraphs)

    story_html_parts = []
    image_paths = []

    # åˆå§‹çº¯æ–‡å­—
    for i, para in enumerate(paragraphs):
        story_html_parts.append(f'<div id="para{i}"><p>{para}</p></div>')

    yield "\n".join(story_html_parts), "å¼€å§‹ç”Ÿæˆæ’å›¾...", None, None, None, None

    for idx, prompt in enumerate(progress.tqdm(english_prompts, desc="ç”Ÿæˆæ’å›¾ä¸­...")):
        img_path = f"illustration_{idx + 1}.jpg"
        try:
            generate_image(prompt, img_path)
            img = Image.open(img_path)
            img.thumbnail((500, 500))
            img.save(img_path)
            image_paths.append(img_path)

            # æ’å…¥å›¾åƒ HTML åˆ°å¯¹åº”æ®µè½
            # story_html_parts[idx] = story_html_parts[idx].replace(
            #     "</div>", f'<img src="file/{img_path}" style="max-width:100%; margin-top:5px;"></div>'
            # )
            base64_src = image_to_base64(img_path)
            story_html_parts[idx] = story_html_parts[idx].replace(
                "</div>",
                f'<img src="{base64_src}" style="max-width:100%; margin-top:5px;"></div>'
            )

        except Exception as e:
            story_html_parts[idx] += f"<p style='color:red;'>æ’å›¾å¤±è´¥ï¼š{e}</p>"

        yield "\n".join(story_html_parts), f"âœ… å·²å®Œæˆ {idx+1}/{len(english_prompts)} å¼ æ’å›¾", None, None, None, None

    # å››ç§ PDF åˆ†åˆ«ç”Ÿæˆ
    pdf_story = create_pdf(data, mode="story")  # ä»…æ­£æ–‡ï¼Œæ— å›¾
    pdf_full = create_pdf(data, mode="full", image_paths=image_paths)  # å…¨éƒ¨æ–‡å­—ä¿¡æ¯ï¼Œæ— å›¾
    pdf_qa = create_pdf(data, mode="question", image_paths=image_paths)  # é—®ç­”ç»ƒä¹ ï¼Œæ— å›¾
    pdf_illustrated = create_pdf(data, mode="illustrated", image_paths=image_paths)  # å›¾æ–‡ç‰ˆï¼Œåªæ­£æ–‡+å›¾

    yield "\n".join(story_html_parts), "âœ… æ‰€æœ‰æ’å›¾å·²å®Œæˆ", pdf_story, pdf_full, pdf_qa, pdf_illustrated

# ============== Gradio UI =================

with gr.Blocks(title="å›¾æ–‡ç»˜æœ¬ç”Ÿæˆç³»ç»Ÿ") as demo:
    gr.Markdown("## ğŸ¨ å­¤ç‹¬ç—‡å„¿ç«¥å›¾æ–‡ç»˜æœ¬ç”Ÿæˆå™¨")

    with gr.Row():
        with gr.Column():
            age = gr.Slider(2, 18, step=1, label="å„¿ç«¥å¹´é¾„", value=5)
            trait = gr.Textbox(label="æ€§æ ¼ç‰¹ç‚¹ï¼ˆå¦‚ å®³ç¾ã€å–œæ¬¢é‡å¤ï¼‰")
            interest = gr.Textbox(label="å…´è¶£ä¸»é¢˜ï¼ˆå¦‚ ç«è½¦ã€æé¾™ï¼‰")
            sensitivity = gr.Dropdown(["å£°éŸ³æ•æ„Ÿ", "è§¦è§‰æ•æ„Ÿ", "æ— "], label="æ„Ÿå®˜æ•æ„Ÿç‚¹")
            cognitive = gr.Dropdown(["åˆçº§", "ä¸­çº§", "é«˜çº§"], label="è®¤çŸ¥æ°´å¹³")
            max_words = gr.Slider(50, 1500, step=10, label="æ•…äº‹å­—æ•°", value=400)
            max_characters = gr.Slider(1, 5, step=1, value=3, label="æœ€å¤šè§’è‰²æ•°é‡")
            edu_goal = gr.Dropdown(
                ["ç¤¾äº¤äº’åŠ¨ï¼šé€šè¿‡ç©ç§¯æœ¨å­¦ä¼šè½®æµ", "æƒ…ç»ªè®¤çŸ¥ï¼šé€šè¿‡æ•…äº‹å¼•å¯¼æƒ…ç»ªå®‰æŠš", "è¡Œä¸ºè§„èŒƒï¼šé€šè¿‡å…¬å›­åœºæ™¯å­¦ä¹ ä¸æ‰“äºº", "æ„Ÿå®˜è°ƒèŠ‚ï¼šå¸®åŠ©å­©å­è¯†åˆ«å®‰é™ç©ºé—´", "å­¦æ ¡æƒ…å¢ƒï¼šå­¦ä¹ ä¸è€å¸ˆè¡¨è¾¾éœ€æ±‚", "è‡ªå®šä¹‰ç›®æ ‡"],
                label="æ•™è‚²ç›®æ ‡ï¼ˆå¯è‡ªå®šä¹‰ï¼‰")
            edu_goal_custom = gr.Textbox(label="å¦‚é€‰æ‹©è‡ªå®šä¹‰ï¼Œè¯·åœ¨æ­¤å¡«å†™ç›®æ ‡è¯´æ˜")
            generate_btn = gr.Button("ç”Ÿæˆå›¾æ–‡ç»˜æœ¬")

        with gr.Column():
            title_box = gr.Textbox(label="ç»˜æœ¬æ ‡é¢˜")
            # story_box = gr.Textbox(label="æ•…äº‹æ­£æ–‡", lines=10)
            story_display = gr.HTML(label="æ•…äº‹æ­£æ–‡ï¼ˆå›¾æ–‡ï¼‰")
            goal_box = gr.Textbox(label="æ•™è‚²ç›®æ ‡")
            tip_box = gr.Textbox(label="å®¶é•¿å¼•å¯¼å»ºè®®")
            qa_box = gr.Textbox(label="ç†è§£é—®ç­”", lines=4)
            # image_gallery = gr.Dataset(components=[gr.Textbox(visible=True, label="æ®µè½"), gr.Image(label="æ’å›¾")],
            #                            label="ğŸ“– å›¾æ–‡å±•ç¤ºåŒº")
            image_progress = gr.Textbox(label="ç”Ÿæˆè¿›åº¦", interactive=False)

    with gr.Row():
        with gr.Column():
            pdf_file_story = gr.File(label="ğŸ“˜ ç»˜æœ¬PDFï¼ˆä»…æ­£æ–‡ï¼‰")
            pdf_file_full = gr.File(label="ğŸ“• å®Œæ•´PDFï¼ˆå«ç›®æ ‡ä¸å»ºè®®ï¼‰")
            pdf_file_qa = gr.File(label="ğŸ§  ç»ƒä¹ ç‰ˆPDFï¼ˆé™„å¸¦é—®ç­”ï¼‰")
            pdf_file_illustrated = gr.File(label="ğŸ¨ å›¾æ–‡ç‰ˆPDFï¼ˆæ¯æ®µé…å›¾ï¼‰")

    intermediate_state = gr.State()

    generate_btn.click(
        fn=generate_story_only,
        inputs=[age, trait, interest, sensitivity, cognitive, max_words, max_characters, edu_goal, edu_goal_custom],
        outputs=[title_box, story_display, goal_box, tip_box, qa_box, intermediate_state]
    )

    intermediate_state.change(
        fn=async_generate_images_and_pdfs,
        inputs=[intermediate_state],
        outputs=[story_display, image_progress, pdf_file_story, pdf_file_full, pdf_file_qa, pdf_file_illustrated],
        show_progress=True
    )

    demo.launch()
